{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [13/Jun/2023 18:27:57] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import KDTree\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(_name_)\n",
    "\n",
    "# Load the models\n",
    "institution_model = load_model('institution_model.h5')\n",
    "sound_model = load_model('sound_model.h5')\n",
    "\n",
    "# Load the datasets\n",
    "df_institution = pd.read_csv('data.csv')\n",
    "df_sound = pd.read_csv('Train set Data Capstone Bangkit.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_institution[['Latitude', 'Longitude']])\n",
    "kdtree = KDTree(scaler.transform(df_institution[['Latitude', 'Longitude']]))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_sound['Kalimat'].astype(str).tolist())\n",
    "word_index = tokenizer.word_index\n",
    "max_sequence_length = max([len(seq) for seq in tokenizer.texts_to_sequences(df_sound['Kalimat'].astype(str).tolist())])\n",
    "\n",
    "labels_dict = {'Polisi': 0, 'RS': 1, 'Damkar': 2, 'PMI': 3, 'Basarnas': 4}\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    latitude = data['Latitude']\n",
    "    longitude = data['Longitude']\n",
    "    new_sentences = [data['bahaya anda']]\n",
    "\n",
    "    new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "    new_padded_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "    sound_prediction = sound_model.predict(new_padded_sequences)\n",
    "    sound_label = list(labels_dict.keys())[np.argmax(sound_prediction)]\n",
    "\n",
    "    location = scaler.transform([[latitude, longitude]])\n",
    "    _, indices = kdtree.query(location, k=5)\n",
    "    nearest_institutions = df_institution.loc[indices[0], :]\n",
    "    nearest_institutions = nearest_institutions[nearest_institutions['Label'] == sound_label]\n",
    "\n",
    "    response_data = []\n",
    "    for _, row in nearest_institutions.iterrows():\n",
    "        institution_data = {\n",
    "            'ID': row['ID'],\n",
    "            'KETERANGAN': row['KETERANGAN'],\n",
    "            'TLP': row['TLP'],\n",
    "            'Latitude': row['Latitude'],\n",
    "            'Longitude': row['Longitude'],\n",
    "            'Label': row['Label']\n",
    "        }\n",
    "        response_data.append(institution_data)\n",
    "        break\n",
    "\n",
    "    if nearest_institutions.empty:\n",
    "        all_institutions = df_institution[df_institution['Label'] == sound_label]\n",
    "        if all_institutions.empty:\n",
    "            response_data = \"No institutions found for the specified type.\"\n",
    "        else:\n",
    "            all_institutions_data = []\n",
    "            for _, row in all_institutions.iterrows():\n",
    "                institution_data = {\n",
    "                    'KETERANGAN': row['KETERANGAN'],\n",
    "                    'TLP': row['TLP'],\n",
    "                    'Latitude': row['Latitude'],\n",
    "                    'Longitude': row['Longitude'],\n",
    "                    'Label': row['Label']\n",
    "                }\n",
    "                all_institutions_data.append(institution_data)\n",
    "                break\n",
    "            response_data = all_institutions_data\n",
    "            \n",
    "\n",
    "    return jsonify(response_data)\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42392ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
